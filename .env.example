# =============================================================================
# LLM PROVIDER CONFIGURATION
# Choose one or multiple providers. GitTalker supports all major LLM services.
# =============================================================================

# OpenAI Configuration (GPT-4, GPT-3.5-turbo, etc.)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic Configuration (Claude 3.5, Claude 3, etc.)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_BASE_URL=https://api.anthropic.com

# Local LLM Configuration (Ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-coder:33b
# Alternative Ollama models: llama3.1:8b, codellama:13b, mistral:7b

# vLLM Configuration (Self-hosted or cloud)
VLLM_BASE_URL=http://localhost:8000
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_API_KEY=optional_api_key_if_required

# =============================================================================
# PRIMARY LLM SELECTION
# Set which provider to use as primary (fallback to others if needed)
# Options: openai, anthropic, ollama, vllm
# =============================================================================
PRIMARY_LLM_PROVIDER=openai

# =============================================================================
# LLM GENERAL SETTINGS
# =============================================================================
TEMPERATURE=0.7
MAX_CONTEXT_LENGTH=4000
MAX_TOKENS=2000

# =============================================================================
# GITHUB INTEGRATION
# =============================================================================
# GitHub Configuration
GITHUB_TOKEN=your_github_personal_access_token_here
GITHUB_REPO=your_username/your_repo_name
GITHUB_DOCS_PATH=gittalker/

# =============================================================================
# SLACK INTEGRATION
# =============================================================================
# Slack Configuration
SLACK_BOT_TOKEN=xoxb-your-bot-token-here
SLACK_APP_TOKEN=xapp-your-app-token-here

# =============================================================================
# AGENT CONFIGURATION
# =============================================================================
# Agent Configuration
AGENT_NAME=GitTalker

# =============================================================================
# MEETING INTELLIGENCE INTEGRATION (Future)
# =============================================================================
# Note: Cluely doesn't currently offer public APIs, but this shows vision
# for future integration when meeting intelligence platforms open up

# Cluely Meeting Intelligence (Enterprise Only - No Public API Currently)
# CLUELY_TEAM_ID=your_enterprise_team_id_here
# CLUELY_API_KEY=not_available_public_api
# CLUELY_WEBHOOK_URL=not_currently_supported

# Alternative Meeting Intelligence Options
# Otter.ai API (if available)
# OTTER_API_KEY=your_otter_api_key_here

# Zoom App Integration (for meeting context)
# ZOOM_CLIENT_ID=your_zoom_client_id_here
# ZOOM_CLIENT_SECRET=your_zoom_client_secret_here

# Microsoft Teams Webhook (for meeting notifications)  
# TEAMS_WEBHOOK_URL=https://your-org.webhook.office.com/webhookb2/...

# =============================================================================
# ADVANCED LLM SETTINGS
# =============================================================================

# Model-specific settings (uncomment and configure as needed)
# OPENAI_ORGANIZATION=your_openai_org_id
# ANTHROPIC_VERSION=2023-06-01
# OLLAMA_KEEP_ALIVE=5m
# VLLM_TRUST_REMOTE_CODE=true

# Rate limiting and retry settings
MAX_REQUESTS_PER_MINUTE=60
REQUEST_TIMEOUT=30
MAX_RETRIES=3

# Fallback behavior
ENABLE_LLM_FALLBACK=true
FALLBACK_ORDER=openai,anthropic,ollama

# =============================================================================
# QUICK SETUP EXAMPLES
# =============================================================================

# Example 1: OpenAI Only
# PRIMARY_LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_MODEL=gpt-4o-mini

# Example 2: Local Ollama Only  
# PRIMARY_LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=deepseek-coder:33b

# Example 3: Anthropic with OpenAI Fallback
# PRIMARY_LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# ENABLE_LLM_FALLBACK=true
# FALLBACK_ORDER=anthropic,openai

# Example 4: Self-hosted vLLM
# PRIMARY_LLM_PROVIDER=vllm
# VLLM_BASE_URL=https://your-vllm-server.com
# VLLM_MODEL=meta-llama/Llama-3.1-70B-Instruct